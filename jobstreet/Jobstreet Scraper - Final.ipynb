{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1481afda",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64246f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99cde3",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32f4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver import Chrome\n",
    "from selenium import webdriver\n",
    "import re \n",
    "import time\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87309609",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('headless')\n",
    "driver = webdriver.Chrome(options=op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e827860f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# path = \"\\jobstreet\\chromedriver_win32\"\n",
    "# WINDOW_SIZE = \"1920,1080\"\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "# chrome_options.binary_location = path\n",
    "\n",
    "# driver = webdriver.Chrome(executable_path=path,\n",
    "#                           chrome_options=chrome_options\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e4aa67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path = \"\\jobstreet\\chromedriver_win32\"\n",
    "# driver = Chrome(executable_path=path)\n",
    "# base_url = \"https://www.jobstreet.com.sg/en/job-search/{}-jobs/{}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aacbc8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d3c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75477b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_number(keyword):\n",
    "    #input: keyword for job_postings\n",
    "    #output: number of pages\n",
    "\n",
    "    url = base_url.format(keyword, 1)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #Finds the number of search results (Page and Total)\n",
    "    result_text = soup.find(\"span\",{\"class\": \"sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7\"})\n",
    "    \n",
    "    #Splits the search results into a list\n",
    "    results = result_text.text.split()\n",
    "    \n",
    "    #Replace comma from result and gets the total number of results returned\n",
    "    result = int(result_text.text.split()[-2].replace(',', ''))\n",
    "    \n",
    "    #Gets the number of pages\n",
    "    page_number = math.ceil(result/30)\n",
    "    \n",
    "    #Returns total number of pages\n",
    "    return page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b18f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_page_scraper(link):\n",
    "\n",
    "    url = \"https://www.jobstreet.com.sg\"+link\n",
    "    print(\"scraping...\", url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    scripts = soup.find_all(\"script\")\n",
    "\n",
    "    for script in scripts:\n",
    "        if script.contents:\n",
    "            txt = script.contents[0].strip()\n",
    "            if 'window.REDUX_STATE = ' in txt:\n",
    "                jsonStr = script.contents[0].strip()\n",
    "                jsonStr = jsonStr.split('window.REDUX_STATE = ')[1].strip()\n",
    "                jsonStr = jsonStr.split('}}}};')[0].strip()\n",
    "                jsonStr = jsonStr+\"}}}}\"\n",
    "                jsonObj = json.loads(jsonStr)\n",
    "    \n",
    "    job = jsonObj['details']\n",
    "    \n",
    "    if(job['id']!=''):\n",
    "        try:\n",
    "            job_salary_min = job['header']['salary']['min']\n",
    "            job_salary_max = job['header']['salary']['max']\n",
    "            job_salary_currency = job['header']['salary']['currency']\n",
    "        except Exception:\n",
    "            job_salary_min =''\n",
    "            job_salary_max = ''\n",
    "            job_salary_currency = ''\n",
    "\n",
    "        job_title = job['header']['jobTitle']\n",
    "        company = job['header']['company']['name']\n",
    "        job_post_date = job['header']['postedDate']\n",
    "        job_internship = job['header']['isInternship']\n",
    "        company_overview = job['companyDetail']['companyOverview']['html']\n",
    "        company_overview = remove_html_tags(company_overview)\n",
    "        job_description = job['jobDetail']['jobDescription']['html']\n",
    "        #Remove html tags\n",
    "        job_description = remove_html_tags(job_description)\n",
    "        job_requirement_career_level = job['jobDetail']['jobRequirement']['careerLevel']\n",
    "        job_requirement_yearsOfExperience = job['jobDetail']['jobRequirement']['yearsOfExperience']\n",
    "        job_requirement_qualification = job['jobDetail']['jobRequirement']['qualification']\n",
    "        job_employment_type = job['jobDetail']['jobRequirement']['employmentType']\n",
    "        job_apply_url = job['applyUrl']['url']\n",
    "        job_location = job['location'][0]['location']\n",
    "        job_country = job['sourceCountry']\n",
    "\n",
    "        return [job_title, job_salary_min, job_salary_max, job_salary_currency, company, job_post_date, job_internship, company_overview, job_description, job_requirement_career_level, job_requirement_yearsOfExperience, job_requirement_qualification, job_employment_type, job_apply_url, job_location, job_country]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82994095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_crawler(keyword):\n",
    "    # input: keyword for job postings\n",
    "    # output: dataframe of links scraped from each page\n",
    "\n",
    "    # page number\n",
    "    page_number = get_page_number(keyword)\n",
    "    job_links = []\n",
    "\n",
    "    for n in range(page_number):\n",
    "        print('Loading page {} ...'.format(n+1))\n",
    "        url = base_url.format(keyword, n+1)\n",
    "        #Load URL\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "        #extract all job links\n",
    "        links = soup.find_all('a',{'rel':'nofollow noopener noreferrer'})\n",
    "        job_links += links\n",
    " \n",
    "    jobs = []\n",
    "\n",
    "    for link in job_links:\n",
    "        job_link = link['href'].strip().split('?', 1)[0]\n",
    "        jobs.append(job_page_scraper(job_link))\n",
    "    \n",
    "    #Creates dataframe with jobs as values, and columns as column names\n",
    "    result_df = pd.DataFrame(jobs, columns = [\"job_title\", \"job_salary_min\", \"job_salary_max\", \"job_salary_currency\", \"company\", \"job_post_date\", \"job_internship\", \"company_overview\", \"job_description\", \"job_requirement_career_level\", \"job_requirement_yearsOfExperience\", \"job_requirement_qualification\", \"job_employment_type\", \"job_apply_url\", \"job_location\", \"job_country\"])\n",
    "    return result_df\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     # a list of job roles to be crawled\n",
    "#     key_words = ['frontend ux developer morgan']\n",
    "#     dfs = []\n",
    "\n",
    "#     for key in key_words:\n",
    "#         key_df = page_crawler(key)\n",
    "#         dfs.append(key_df)\n",
    "\n",
    "#     # save scraped information as csv\n",
    "#     pd.concat(dfs).to_csv(\"job_postings_results.csv\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46264692",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Search Term: frontend ux developer DBS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20296/3287923086.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mkey_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20296/2130664627.py\u001b[0m in \u001b[0;36mpage_crawler\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# page number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpage_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_page_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mjob_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20296/4003765753.py\u001b[0m in \u001b[0;36mget_page_number\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#output: number of pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_url' is not defined"
     ]
    }
   ],
   "source": [
    "#Request keyword\n",
    "search_term = input(\"Enter Search Term: \")\n",
    "\n",
    "# a list of job roles to be crawled\n",
    "# key_words = ['frontend ux developer DBS']\n",
    "key_words = [search_term]\n",
    "dfs = []\n",
    "\n",
    "for key in key_words:\n",
    "    key_df = page_crawler(key)\n",
    "    dfs.append(key_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afe08c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf5b68",
   "metadata": {},
   "source": [
    "## Convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fd732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export as JSON\n",
    "key_df.to_json('dataframe2.json', orient='records')\n",
    "\n",
    "#Get as dictionary\n",
    "key_df3 = key_df.to_json(orient='records')\n",
    "result_dict = json.loads(key_df3)\n",
    "# result_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cda2ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf577d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
