{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1481afda",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64246f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99cde3",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32f4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver import Chrome\n",
    "import re \n",
    "import time\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e4aa67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iansi\\AppData\\Local\\Temp/ipykernel_3144/2566066071.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = Chrome(executable_path=path)\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "path = \"\\jobstreet\\chromedriver_win32\"\n",
    "\n",
    "driver = Chrome(executable_path=path)\n",
    "time.sleep(2)\n",
    "base_url = \"https://www.jobstreet.com.sg/en/job-search/{}-jobs/{}/\"\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def get_page_number(keyword):\n",
    "    #input: keyword for job_postings\n",
    "    #output: number of pages\n",
    "\n",
    "    url = base_url.format(keyword, 1)\n",
    "    #print(url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #Finds the number of search results (Page and Total)\n",
    "    result_text = soup.find(\"span\",{\"class\": \"sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7\"})\n",
    "    print(result_text)\n",
    "    \n",
    "    #Splits the search results into a list\n",
    "    results = result_text.text.split()\n",
    "    print(results)\n",
    "    \n",
    "    #Replace comma from result and gets the total number of results returned\n",
    "    result = int(result_text.text.split()[-2].replace(',', ''))\n",
    "    page_number = math.ceil(result/30)\n",
    "    \n",
    "    return page_number\n",
    "\n",
    "def job_page_scraper(link):\n",
    "\n",
    "    url = \"https://www.jobstreet.com.sg\"+link\n",
    "    #print(\"scraping...\", url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    scripts = soup.find_all(\"script\")\n",
    "    #print(scripts)\n",
    "\n",
    "    for script in scripts:\n",
    "        if script.contents:\n",
    "            txt = script.contents[0].strip()\n",
    "            if 'window.REDUX_STATE = ' in txt:\n",
    "                jsonStr = script.contents[0].strip()\n",
    "                jsonStr = jsonStr.split('window.REDUX_STATE = ')[1].strip()\n",
    "                jsonStr = jsonStr.split('}}}};')[0].strip()\n",
    "                jsonStr = jsonStr+\"}}}}\"\n",
    "                jsonObj = json.loads(jsonStr)\n",
    "    \n",
    "    job = jsonObj['details']\n",
    "    #print(job)\n",
    "    \n",
    "    if(job['id']!=''):    \n",
    "        #job_id = job['id']\n",
    "        #print(job_id)\n",
    "        #job_expired = job['isExpired']\n",
    "        #job_confidential = job['isConfidential']\n",
    "\n",
    "        try:\n",
    "            job_salary_min = job['header']['salary']['min']\n",
    "            job_salary_max = job['header']['salary']['max']\n",
    "            job_salary_currency = job['header']['salary']['currency']\n",
    "        except Exception:\n",
    "            job_salary_min =''\n",
    "            job_salary_max = ''\n",
    "            job_salary_currency = ''\n",
    "\n",
    "        job_salary_min = job['header']['salary']['min']\n",
    "        job_salary_max = job['header']['salary']['max']\n",
    "        job_salary_currency = job['header']['salary']['currency']\n",
    "\n",
    "        job_title = job['header']['jobTitle']\n",
    "\n",
    "        company = job['header']['company']['name']\n",
    "        #print(company)\n",
    "        job_post_date = job['header']['postedDate']\n",
    "        job_internship = job['header']['isInternship']\n",
    "        #company_website = job['companyDetail']['companyWebsite']\n",
    "        #company_avgProcessTime = job['companyDetail']['companySnapshot']['avgProcessTime']\n",
    "        #company_registrationNo = job['companyDetail']['companySnapshot']['registrationNo']\n",
    "        #company_workingHours = job['companyDetail']['companySnapshot']['workingHours']\n",
    "        #company_facebook = job['companyDetail']['companySnapshot']['facebook']\n",
    "        #company_size = job['companyDetail']['companySnapshot']['size']\n",
    "        #company_dressCode = job['companyDetail']['companySnapshot']['dressCode']\n",
    "        #company_nearbyLocations = job['companyDetail']['companySnapshot']['nearbyLocations']\n",
    "        company_overview = job['companyDetail']['companyOverview']['html']\n",
    "        company_overview = remove_html_tags(company_overview)\n",
    "        \n",
    "        job_description = job['jobDetail']['jobDescription']['html']\n",
    "        #Remove html tags\n",
    "        job_description = remove_html_tags(job_description)\n",
    "        \n",
    "        \n",
    "        #job_summary = job['jobDetail']['summary']\n",
    "        job_requirement_career_level = job['jobDetail']['jobRequirement']['careerLevel']\n",
    "        job_requirement_yearsOfExperience = job['jobDetail']['jobRequirement']['yearsOfExperience']\n",
    "        job_requirement_qualification = job['jobDetail']['jobRequirement']['qualification']\n",
    "        #job_requirement_fieldOfStudy = job['jobDetail']['jobRequirement']['fieldOfStudy']\n",
    "        #job_requirement_industry = job['jobDetail']['jobRequirement']['industryValue']['label']\n",
    "        #job_requirement_skill = job['jobDetail']['jobRequirement']['skills']\n",
    "        job_employment_type = job['jobDetail']['jobRequirement']['employmentType']\n",
    "        #job_languages = job['jobDetail']['jobRequirement']['languages']\n",
    "        #job_benefits = job['jobDetail']['jobRequirement']['benefits']\n",
    "        job_apply_url = job['applyUrl']['url']\n",
    "        #job_location_zipcode = job['location'][0]['locationId']\n",
    "        job_location = job['location'][0]['location']\n",
    "        job_country = job['sourceCountry']\n",
    "\n",
    "        return [job_title, job_salary_min, job_salary_max, job_salary_currency, company, job_post_date, job_internship, company_overview, job_description, job_requirement_career_level, job_requirement_yearsOfExperience, job_requirement_qualification, job_employment_type, job_apply_url, job_location, job_country]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def page_crawler(keyword):\n",
    "    # input: keyword for job postings\n",
    "    # output: dataframe of links scraped from each page\n",
    "\n",
    "    # page number\n",
    "    page_number = get_page_number(keyword)\n",
    "    job_links = []\n",
    "\n",
    "    for n in range(page_number):\n",
    "        print('Loading page {} ...'.format(n+1))\n",
    "        url = base_url.format(keyword, n+1)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "        #extract all job links\n",
    "        links = soup.find_all('a',{'class':'_1hr6tkx5 _1hr6tkx7 _1hr6tkxa sx2jih0 sx2jihf zcydq8h'})\n",
    "        job_links += links\n",
    " \n",
    "    jobs = []\n",
    "\n",
    "    for link in job_links:\n",
    "        job_link = link['href'].strip().split('?', 1)[0]\n",
    "        #Combine the search keyword and job link as the first two columns\n",
    "        if(len(job_page_scraper(job_link))):\n",
    "            jobs.append(job_page_scraper(job_link))\n",
    "    \n",
    "    #Creates dataframe with jobs as values, and columns as column names\n",
    "    result_df = pd.DataFrame(jobs, columns = [\"job_title\", \"job_salary_min\", \"job_salary_max\", \"job_salary_currency\", \"company\", \"job_post_date\", \"job_internship\", \"company_overview\", \"job_description\", \"job_requirement_career_level\", \"job_requirement_yearsOfExperience\", \"job_requirement_qualification\", \"job_employment_type\", \"job_apply_url\", \"job_location\", \"job_country\"])\n",
    "    return result_df\n",
    "\n",
    "def main():\n",
    "\n",
    "    # a list of job roles to be crawled\n",
    "    key_words = ['frontend ux developer morgan']\n",
    "    dfs = []\n",
    "\n",
    "    for key in key_words:\n",
    "        key_df = page_crawler(key)\n",
    "        dfs.append(key_df)\n",
    "\n",
    "    # save scraped information as csv\n",
    "    pd.concat(dfs).to_csv(\"job_postings_results.csv\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279f987",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7\"><strong class=\"es8sxo3\">1-4</strong> of 6 jobs </span>\n",
      "['1-4', 'of', '6', 'jobs']\n",
      "Loading page 1 ...\n"
     ]
    }
   ],
   "source": [
    "# a list of job roles to be crawled\n",
    "key_words = ['frontend ux developer DBS']\n",
    "dfs = []\n",
    "\n",
    "for key in key_words:\n",
    "    key_df = page_crawler(key)\n",
    "    dfs.append(key_df)\n",
    "\n",
    "# save scraped information as csv\n",
    "#pd.concat(dfs).to_csv(\"job_postings_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67492c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_salary_min</th>\n",
       "      <th>job_salary_max</th>\n",
       "      <th>job_salary_currency</th>\n",
       "      <th>company</th>\n",
       "      <th>job_post_date</th>\n",
       "      <th>job_internship</th>\n",
       "      <th>company_overview</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_requirement_career_level</th>\n",
       "      <th>job_requirement_yearsOfExperience</th>\n",
       "      <th>job_requirement_qualification</th>\n",
       "      <th>job_employment_type</th>\n",
       "      <th>job_apply_url</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [job_title, job_salary_min, job_salary_max, job_salary_currency, company, job_post_date, job_internship, company_overview, job_description, job_requirement_career_level, job_requirement_yearsOfExperience, job_requirement_qualification, job_employment_type, job_apply_url, job_location, job_country]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
