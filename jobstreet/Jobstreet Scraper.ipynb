{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1481afda",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64246f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99cde3",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b32f4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver import Chrome\n",
    "import re \n",
    "import time\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04e4aa67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iansi\\AppData\\Local\\Temp/ipykernel_3144/1774858442.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = Chrome(executable_path=path)\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "path = \"\\jobstreet\\chromedriver_win32\"\n",
    "\n",
    "driver = Chrome(executable_path=path)\n",
    "time.sleep(2)\n",
    "base_url = \"https://www.jobstreet.com.sg/en/job-search/{}-jobs/{}/\"\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def get_page_number(keyword):\n",
    "    #input: keyword for job_postings\n",
    "    #output: number of pages\n",
    "\n",
    "    url = base_url.format(keyword, 1)\n",
    "    #print(url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #Finds the number of search results (Page and Total)\n",
    "    result_text = soup.find(\"span\",{\"class\": \"sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7\"})\n",
    "    print(result_text)\n",
    "    \n",
    "    #Splits the search results into a list\n",
    "    results = result_text.text.split()\n",
    "    print(results)\n",
    "    \n",
    "    #Replace comma from result and gets the total number of results returned\n",
    "    result = int(result_text.text.split()[-2].replace(',', ''))\n",
    "    page_number = math.ceil(result/30)\n",
    "    \n",
    "    return page_number\n",
    "\n",
    "def job_page_scraper(link):\n",
    "\n",
    "    url = \"https://www.jobstreet.com.sg\"+link\n",
    "    #print(\"scraping...\", url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    scripts = soup.find_all(\"script\")\n",
    "    #print(scripts)\n",
    "\n",
    "    for script in scripts:\n",
    "        if script.contents:\n",
    "            txt = script.contents[0].strip()\n",
    "            if 'window.REDUX_STATE = ' in txt:\n",
    "                jsonStr = script.contents[0].strip()\n",
    "                jsonStr = jsonStr.split('window.REDUX_STATE = ')[1].strip()\n",
    "                jsonStr = jsonStr.split('}}}};')[0].strip()\n",
    "                jsonStr = jsonStr+\"}}}}\"\n",
    "                jsonObj = json.loads(jsonStr)\n",
    "    \n",
    "    job = jsonObj['details']\n",
    "    #print(job)\n",
    "    \n",
    "    if(job['id']!=''):    \n",
    "        #job_id = job['id']\n",
    "        #print(job_id)\n",
    "        #job_expired = job['isExpired']\n",
    "        #job_confidential = job['isConfidential']\n",
    "\n",
    "        try:\n",
    "            job_salary_min = job['header']['salary']['min']\n",
    "            job_salary_max = job['header']['salary']['max']\n",
    "            job_salary_currency = job['header']['salary']['currency']\n",
    "        except Exception:\n",
    "            job_salary_min =''\n",
    "            job_salary_max = ''\n",
    "            job_salary_currency = ''\n",
    "\n",
    "        job_salary_min = job['header']['salary']['min']\n",
    "        job_salary_max = job['header']['salary']['max']\n",
    "        job_salary_currency = job['header']['salary']['currency']\n",
    "\n",
    "        job_title = job['header']['jobTitle']\n",
    "\n",
    "        company = job['header']['company']['name']\n",
    "        #print(company)\n",
    "        job_post_date = job['header']['postedDate']\n",
    "        job_internship = job['header']['isInternship']\n",
    "        #company_website = job['companyDetail']['companyWebsite']\n",
    "        #company_avgProcessTime = job['companyDetail']['companySnapshot']['avgProcessTime']\n",
    "        #company_registrationNo = job['companyDetail']['companySnapshot']['registrationNo']\n",
    "        #company_workingHours = job['companyDetail']['companySnapshot']['workingHours']\n",
    "        #company_facebook = job['companyDetail']['companySnapshot']['facebook']\n",
    "        #company_size = job['companyDetail']['companySnapshot']['size']\n",
    "        #company_dressCode = job['companyDetail']['companySnapshot']['dressCode']\n",
    "        #company_nearbyLocations = job['companyDetail']['companySnapshot']['nearbyLocations']\n",
    "        company_overview = job['companyDetail']['companyOverview']['html']\n",
    "        company_overview = remove_html_tags(company_overview)\n",
    "        \n",
    "        job_description = job['jobDetail']['jobDescription']['html']\n",
    "        #Remove html tags\n",
    "        job_description = remove_html_tags(job_description)\n",
    "        \n",
    "        \n",
    "        #job_summary = job['jobDetail']['summary']\n",
    "        job_requirement_career_level = job['jobDetail']['jobRequirement']['careerLevel']\n",
    "        job_requirement_yearsOfExperience = job['jobDetail']['jobRequirement']['yearsOfExperience']\n",
    "        job_requirement_qualification = job['jobDetail']['jobRequirement']['qualification']\n",
    "        #job_requirement_fieldOfStudy = job['jobDetail']['jobRequirement']['fieldOfStudy']\n",
    "        #job_requirement_industry = job['jobDetail']['jobRequirement']['industryValue']['label']\n",
    "        #job_requirement_skill = job['jobDetail']['jobRequirement']['skills']\n",
    "        job_employment_type = job['jobDetail']['jobRequirement']['employmentType']\n",
    "        #job_languages = job['jobDetail']['jobRequirement']['languages']\n",
    "        #job_benefits = job['jobDetail']['jobRequirement']['benefits']\n",
    "        job_apply_url = job['applyUrl']['url']\n",
    "        #job_location_zipcode = job['location'][0]['locationId']\n",
    "        job_location = job['location'][0]['location']\n",
    "        job_country = job['sourceCountry']\n",
    "\n",
    "        return [job_title, job_salary_min, job_salary_max, job_salary_currency, company, job_post_date, job_internship, company_overview, job_description, job_requirement_career_level, job_requirement_yearsOfExperience, job_requirement_qualification, job_employment_type, job_apply_url, job_location, job_country]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def page_crawler(keyword):\n",
    "    # input: keyword for job postings\n",
    "    # output: dataframe of links scraped from each page\n",
    "\n",
    "    # page number\n",
    "    page_number = get_page_number(keyword)\n",
    "    job_links = []\n",
    "\n",
    "    for n in range(page_number):\n",
    "        print('Loading page {} ...'.format(n+1))\n",
    "        url = base_url.format(keyword, n+1)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "        #extract all job links\n",
    "        links = soup.find_all('a',{'class':'_1hr6tkx5 _1hr6tkx7 _1hr6tkxa sx2jih0 sx2jihf zcydq8h'})\n",
    "        job_links += links\n",
    " \n",
    "    jobs = []\n",
    "\n",
    "    for link in job_links:\n",
    "        job_link = link['href'].strip().split('?', 1)[0]\n",
    "        #Combine the search keyword and job link as the first two columns\n",
    "        if(len(job_page_scraper(job_link))):\n",
    "            jobs.append(job_page_scraper(job_link))\n",
    "    \n",
    "    #Creates dataframe with jobs as values, and columns as column names\n",
    "    result_df = pd.DataFrame(jobs, columns = [\"job_title\", \"job_salary_min\", \"job_salary_max\", \"job_salary_currency\", \"company\", \"job_post_date\", \"job_internship\", \"company_overview\", \"job_description\", \"job_requirement_career_level\", \"job_requirement_yearsOfExperience\", \"job_requirement_qualification\", \"job_employment_type\", \"job_apply_url\", \"job_location\", \"job_country\"])\n",
    "    return result_df\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     # a list of job roles to be crawled\n",
    "#     key_words = ['frontend ux developer morgan']\n",
    "#     dfs = []\n",
    "\n",
    "#     for key in key_words:\n",
    "#         key_df = page_crawler(key)\n",
    "#         dfs.append(key_df)\n",
    "\n",
    "#     # save scraped information as csv\n",
    "#     pd.concat(dfs).to_csv(\"job_postings_results.csv\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cf03792",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Search Term: Frontend UX Developer DBS\n",
      "<span class=\"sx2jih0 zcydq84u es8sxo0 es8sxo1 es8sxo21 _1d0g9qk4 es8sxo7\"><strong class=\"es8sxo3\">1-4</strong> of 6 jobs </span>\n",
      "['1-4', 'of', '6', 'jobs']\n",
      "Loading page 1 ...\n"
     ]
    }
   ],
   "source": [
    "#Request keyword\n",
    "search_term = input(\"Enter Search Term: \")\n",
    "\n",
    "# a list of job roles to be crawled\n",
    "# key_words = ['frontend ux developer DBS']\n",
    "key_words = [search_term]\n",
    "dfs = []\n",
    "\n",
    "for key in key_words:\n",
    "    key_df = page_crawler(key)\n",
    "    dfs.append(key_df)\n",
    "\n",
    "# save scraped information as csv\n",
    "#pd.concat(dfs).to_csv(\"job_postings_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bddedef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_salary_min</th>\n",
       "      <th>job_salary_max</th>\n",
       "      <th>job_salary_currency</th>\n",
       "      <th>company</th>\n",
       "      <th>job_post_date</th>\n",
       "      <th>job_internship</th>\n",
       "      <th>company_overview</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_requirement_career_level</th>\n",
       "      <th>job_requirement_yearsOfExperience</th>\n",
       "      <th>job_requirement_qualification</th>\n",
       "      <th>job_employment_type</th>\n",
       "      <th>job_apply_url</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VP/AVP, - ReactJS Developer, Branch and Self-s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DBS Bank Limited</td>\n",
       "      <td>10-Jan-23</td>\n",
       "      <td>False</td>\n",
       "      <td>DBS is a leading financial services group in A...</td>\n",
       "      <td>Business FunctionAs the leading bank in Asia, ...</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>8 years</td>\n",
       "      <td>Bachelor's Degree, Post Graduate Diploma, Prof...</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>https://careers.dbs.com/careersection/dbs_prof...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VP/AVP, ReactJS Developer, Branch and Self-ser...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DBS Bank Limited</td>\n",
       "      <td>10-Jan-23</td>\n",
       "      <td>False</td>\n",
       "      <td>DBS is a leading financial services group in A...</td>\n",
       "      <td>Business FunctionAs the leading bank in Asia, ...</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>8 years</td>\n",
       "      <td>Bachelor's Degree, Post Graduate Diploma, Prof...</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>https://careers.dbs.com/careersection/dbs_prof...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>Tow Me Sg Pte. Ltd.</td>\n",
       "      <td>11 hours ago</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>Full Stack Developer Job DescriptionAs the ful...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td></td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>https://sg.jobsdb.com/job/rd/8216d70245e46abea...</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Copywriter</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>DDB Worldwide Pte Ltd</td>\n",
       "      <td>14-Jan-23</td>\n",
       "      <td>False</td>\n",
       "      <td>About DDB Group Singapore – DDB Group is one o...</td>\n",
       "      <td>About DDB Group Singapore – DDB Group is one o...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td></td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>https://sg.jobsdb.com/job/rd/f9cba4fc20187b8cc...</td>\n",
       "      <td>Kallang</td>\n",
       "      <td>sg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title job_salary_min  \\\n",
       "0  VP/AVP, - ReactJS Developer, Branch and Self-s...           None   \n",
       "1  VP/AVP, ReactJS Developer, Branch and Self-ser...           None   \n",
       "2                               Full Stack Developer           None   \n",
       "3                                         Copywriter           None   \n",
       "\n",
       "  job_salary_max job_salary_currency                company job_post_date  \\\n",
       "0           None                 SGD       DBS Bank Limited     10-Jan-23   \n",
       "1           None                 SGD       DBS Bank Limited     10-Jan-23   \n",
       "2           None                 SGD    Tow Me Sg Pte. Ltd.  11 hours ago   \n",
       "3           None                 SGD  DDB Worldwide Pte Ltd     14-Jan-23   \n",
       "\n",
       "   job_internship                                   company_overview  \\\n",
       "0           False  DBS is a leading financial services group in A...   \n",
       "1           False  DBS is a leading financial services group in A...   \n",
       "2           False                                                      \n",
       "3           False  About DDB Group Singapore – DDB Group is one o...   \n",
       "\n",
       "                                     job_description  \\\n",
       "0  Business FunctionAs the leading bank in Asia, ...   \n",
       "1  Business FunctionAs the leading bank in Asia, ...   \n",
       "2  Full Stack Developer Job DescriptionAs the ful...   \n",
       "3  About DDB Group Singapore – DDB Group is one o...   \n",
       "\n",
       "  job_requirement_career_level job_requirement_yearsOfExperience  \\\n",
       "0               Senior Manager                           8 years   \n",
       "1               Senior Manager                           8 years   \n",
       "2                Not Specified                                     \n",
       "3                Not Specified                                     \n",
       "\n",
       "                       job_requirement_qualification job_employment_type  \\\n",
       "0  Bachelor's Degree, Post Graduate Diploma, Prof...           Full-Time   \n",
       "1  Bachelor's Degree, Post Graduate Diploma, Prof...           Full-Time   \n",
       "2                                      Not Specified           Full-Time   \n",
       "3                                      Not Specified           Full-Time   \n",
       "\n",
       "                                       job_apply_url job_location job_country  \n",
       "0  https://careers.dbs.com/careersection/dbs_prof...    Singapore          sg  \n",
       "1  https://careers.dbs.com/careersection/dbs_prof...    Singapore          sg  \n",
       "2  https://sg.jobsdb.com/job/rd/8216d70245e46abea...    Singapore          sg  \n",
       "3  https://sg.jobsdb.com/job/rd/f9cba4fc20187b8cc...      Kallang          sg  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c38b4",
   "metadata": {},
   "source": [
    "## Convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ee3d935",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_3144/2617395563.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\iansi\\AppData\\Local\\Temp/ipykernel_3144/2617395563.py\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    for z in range(len(key_df.columns)):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Define Dictionary\n",
    "\n",
    "array = []\n",
    "#Columns\n",
    "for x in range(len(key_df.columns)):\n",
    "    #Rows\n",
    "    for y in range(len(key_df.index)):\n",
    "        pikachu = {\n",
    "            for z in range(len(key_df.columns)):\n",
    "                str(key_df.columns[z]): key_df.iloc[y][z]\n",
    "        }\n",
    "        array.append[pikachu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ecaf592d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VP/AVP, - ReactJS Developer, Branch and Self-serviced Banking, Consumer Banking Group Technology, Technology & Operations - (WD39175) #JobsThatMatter.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e22f09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job_title'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32fbd190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6f9d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = key_df.to_json('dataframe.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "211954f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3144/299554093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "Y = json.load(X)\n",
    "print(Y.job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd41dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
